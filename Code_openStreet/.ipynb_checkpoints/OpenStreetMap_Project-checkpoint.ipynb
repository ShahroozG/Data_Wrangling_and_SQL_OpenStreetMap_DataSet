{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schemaa\n",
    "import string\n",
    "import phonenumbers\n",
    "import requests\n",
    "import validators\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SourceFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSM_FILE = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/santaMonica.osm\"\n",
    "SAMPLE_FILE = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/sampleSantaMonica.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tags Variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags Numbers: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'bounds': 1,\n",
       "             'member': 4634,\n",
       "             'nd': 710878,\n",
       "             'node': 638960,\n",
       "             'osm': 1,\n",
       "             'relation': 1031,\n",
       "             'tag': 436054,\n",
       "             'way': 63953})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting different tags names in the OSM file:\n",
    "# reference: Data Wrangling Course-Udacity\n",
    "def count_tags(filename):\n",
    "    output = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        output[elem.tag] += 1\n",
    "    return output\n",
    "\n",
    "print \"Tags Numbers: \"\n",
    "count_tags(OSM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Contributers in this Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a set that contains contributers(Users) in this Map\n",
    "# reference: Data Wrangling Course-Udacity\n",
    "\n",
    "def process_map_users(filename):\n",
    "    users = set()\n",
    "    \n",
    "    for _, element in ET.iterparse(filename):\n",
    "        try:\n",
    "            users.add(element.attrib['uid'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contributers:  350\n"
     ]
    }
   ],
   "source": [
    "users = process_map_users(OSM_FILE)\n",
    "print 'Number of Contributers: ', len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags Patterns are: \n",
      "{'lower': 249540, 'lower_colon': 184694, 'other': 1819, 'problemchars': 1}\n"
     ]
    }
   ],
   "source": [
    "# looking at tags Pattern and find how many of each we have in our OSM file;\n",
    "# reference: Data Wrangling Course-Udacity\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k_value = element.attrib['k']\n",
    "        \n",
    "        if lower.search(k_value):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(k_value):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(k_value):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "        \n",
    "    return keys\n",
    "\n",
    "# in this function first we put keys as a dictionary with 0 values. So it is initializred and when I use key_type\n",
    "# function I will traverse on all tags and increment the number of each category;\n",
    "\n",
    "def process_map_tag_type(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "keys = process_map_tag_type(OSM_FILE)\n",
    "print \"Tags Patterns are: \"\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of : Street, State, Zip Code, HouseNumber, Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street Types: \n",
      "defaultdict(<type 'int'>, {'Boulevard': 132, 'Sepulveda': 1, 'St.': 1, 'Way': 7, 'broadway': 1, 'avenue': 1, 'Highway': 2, 'Promenade': 5, 'North': 1, 'access': 1, 'Ln': 1, 'South': 1, 'Dr': 3, 'Center': 1, 'Bd.': 1, 'Drive': 4, 'Pico': 1, 'Place': 2, 'Ave': 2, 'Road': 1, 'Blvd.': 1, 'Walk': 4, 'Street': 42, 'Bvd': 1, 'Blvd': 7, 'Broadway': 7, 'Avenue': 49})\n",
      "\n",
      "State Types: \n",
      "defaultdict(<type 'int'>, {'CA': 225, 'Ca': 1, 'CA,': 1})\n",
      "\n",
      "PostCode Types: \n",
      "defaultdict(<type 'int'>, {'90024': 2, '90025': 186, '90401-2405': 1, 'CA 90401': 1, '90066': 3, '90064': 13, '90291-3879': 1, 'CA 90291': 1, 'CA 90272': 1, 'CA 90024': 1, '90064-1508': 1, '90025-9998': 1, '90291': 44, '90292': 5, '90501': 1, 'CA 90405': 3, '90401': 32, '90402': 5, '90403': 20, '90404': 33, '90405': 43, 'CA 90404': 1, '90049': 13, '90272-3719': 1})\n",
      "\n",
      "Problematic House Numbers: \n",
      "['1850 Sawtelle Boulevard, Suite 300, Los Angeles, CA 90025',\n",
      " '12301  Suite 650']\n",
      "\n",
      "Problematic Phone Numbers: \n",
      "['01-310-260-6308']\n",
      "\n",
      "Problematic Website addresses: \n",
      "['http://www.dot.ca.gov/hq/tsip/gis/datalibrary/gisdatalibrary.html, bing']\n"
     ]
    }
   ],
   "source": [
    "# Which Street type are in the file?\n",
    "# Street audit reference: Data Wrangling Course-Udacity\n",
    "\n",
    "osm_file = open(OSM_FILE, \"r\")\n",
    "\n",
    "street_type_re = re.compile(r'\\b([a-z])+\\.?$', re.IGNORECASE)\n",
    "housenumber_type_re = re.compile(r'^\\d+(-?\\d)*$')\n",
    "\n",
    "street_types = defaultdict(int)\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_types[street_type] += 1\n",
    "        \n",
    "state_types = defaultdict(int)\n",
    "def audit_state_type(state_types, state):\n",
    "    state_types[state] += 1        \n",
    "\n",
    "postcode_types = defaultdict(int)\n",
    "def audit_postcode(postcode_types, postcode):\n",
    "    postcode_types[postcode] += 1\n",
    "\n",
    "problemNumbers = []\n",
    "def audit_housenumber(problemNumbers, number):\n",
    "    m = housenumber_type_re.search(number)\n",
    "    if not m:\n",
    "        problemNumbers.append(number)\n",
    "\n",
    "notValidPhones = []\n",
    "def audit_phone(notValidPhones, phone):\n",
    "    if phone.startswith(\"+\"):\n",
    "        phone = phone[1:]\n",
    "    z = phonenumbers.parse(phone, \"US\")\n",
    "    v = phonenumbers.is_possible_number(z)\n",
    "    if not v:\n",
    "        notValidPhones.append(phone)\n",
    "\n",
    "# This function is based on request method and it catches many valid websites; So I don't use it and use the other function\n",
    "# but the idea behind it is good and it's about how to use request method so I keep it here for learning purposes;\n",
    "#problemWebsite = set()\n",
    "#def audit_website(problemWebsite, website): \n",
    "#    if not website.startswith('http'):\n",
    "#        website = 'http://' + website\n",
    "#    try:\n",
    "#        request = requests.get(website)\n",
    "#        if request.status_code != 200:\n",
    "#            problemWebsite.add(website)\n",
    "#    except:\n",
    "#        problemWebsite.add(website)\n",
    "\n",
    "problemWebsite = []\n",
    "def audit_website(problemWebsite, website):\n",
    "    if not website.startswith('http'):\n",
    "        website = 'http://' + website\n",
    "    if not validators.url(website):\n",
    "        problemWebsite.append(website)\n",
    "    \n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def is_state(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:state\" or elem.attrib['k'] == \"is_in:state_code\")\n",
    "\n",
    "def is_postcode(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def is_housenumber(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:housenumber\")\n",
    "\n",
    "def is_phone(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == 'phone')\n",
    "\n",
    "def is_website(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"website\" or elem.attrib['k'] == \"url\" or \\\n",
    "                                    (elem.attrib['k'] == \"source\" and elem.attrib['v'].startswith(\"http\")))\n",
    "\n",
    "\n",
    "def audit():\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "        elif is_state(elem):\n",
    "            audit_state_type(state_types, elem.attrib['v'])\n",
    "        elif is_postcode(elem):\n",
    "            audit_postcode(postcode_types, elem.attrib['v'])\n",
    "        elif is_housenumber(elem):\n",
    "            audit_housenumber(problemNumbers, elem.attrib['v'])\n",
    "        elif is_phone(elem):\n",
    "            audit_phone(notValidPhones, elem.attrib['v'])\n",
    "        elif is_website(elem):\n",
    "            audit_website(problemWebsite, elem.attrib['v'])\n",
    "        \n",
    "    print \"Street Types: \"        \n",
    "    pprint.pprint(street_types)\n",
    "    print\"\"\n",
    "    print \"State Types: \"\n",
    "    pprint.pprint(state_types)\n",
    "    print\"\"\n",
    "    print \"PostCode Types: \"\n",
    "    pprint.pprint(postcode_types)\n",
    "    print\"\"\n",
    "    print \"Problematic House Numbers: \"\n",
    "    pprint.pprint(problemNumbers)\n",
    "    print\"\"\n",
    "    print \"Problematic Phone Numbers: \"\n",
    "    pprint.pprint(notValidPhones)\n",
    "    print\"\"\n",
    "    print \"Problematic Website addresses: \"\n",
    "    pprint.pprint(problemWebsite)\n",
    "\n",
    "audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reference for Street Part: Data Wrangling Course-Udacity\n",
    "street_type_re = re.compile(r'\\b([a-z])+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# catching direction abbreviations at the first of an address: E, W, N, S\n",
    "street_abbrev_re = re.compile(r'^([a-z]){1}\\.?(\\s)+', re.IGNORECASE)\n",
    "\n",
    "# catching house numbers as part of street address\n",
    "housenumber_in_street_re = re.compile(r'^(\\d)+(\\s)+')\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Broadway\", \"Promenade\", \"Way\", \"Highway\", \"Walk\"]\n",
    "\n",
    "# I consider the file and found problematic street names and create this map\n",
    "mapping_street = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Blvd\" : \"Boulevard\",\n",
    "            \"Blvd.\" : \"Boulevard\",\n",
    "            \"Bd.\" : \"Boulevard\",\n",
    "            \"Bvd\" : \"Boulevard\",\n",
    "            \"Dr\" : \"Drive\",\n",
    "            \"Ln\" : \"Lane\",\n",
    "            \"Pico\" : \"Pico Boulevard\",\n",
    "            \"Sepulveda\" : \"Sepulveda Boulevard\",\n",
    "            \"Center\" : \"Center Drive\",\n",
    "            }\n",
    "\n",
    "mapping_abbrev = { 'W ': 'West ', 'S ': 'South ', 'N ': 'North ', 'E ': 'East ',\\\n",
    "                   'W. ': 'West ', 'S. ': 'South', 'N. ': 'North ', 'E. ': 'East '}\n",
    "\n",
    "state_list = [\"Ca\", \"CA,\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    # this is an exception in our street naming: Donald Douglas Loop North and Donald Douglas Loop South are \n",
    "                    # Corrcet addresses and I don't want to change them;\n",
    "                    if not tag.attrib['v'].startswith(\"Donald Douglas Loop\"):\n",
    "                        audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping_street, mapping_abbrev):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        #If the name exist in mapping keys, it means that's a problem and we should fix it.\n",
    "        if street_type in mapping_street.keys():\n",
    "            name = re.sub(street_type, mapping_street[street_type], name)\n",
    "    # Updating W , E, N, S to West, East, North, South; if they are at the begining of an address.\n",
    "    m_1 = street_abbrev_re.search(name)\n",
    "    if m_1:\n",
    "        street_abbrev = m_1.group()\n",
    "        if street_abbrev in mapping_abbrev.keys():\n",
    "            name = re.sub(street_abbrev, mapping_abbrev[street_abbrev], name)\n",
    "    # capitalizing first letter of all words in problematic address\n",
    "    name = string.capwords(name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def update_state(state, state_list):\n",
    "    if state in state_list:\n",
    "        state = \"CA\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ave': set(['Montana Ave', 'Ohio Ave']),\n",
      " 'Bd.': set(['Ocean Bd.']),\n",
      " 'Blvd': set(['Pico Blvd',\n",
      "              'Santa Monica Blvd',\n",
      "              'W Pico Blvd',\n",
      "              'W Washington Blvd',\n",
      "              'Wilshire Blvd',\n",
      "              'Wishire Blvd']),\n",
      " 'Bvd': set(['Santa Monica Bvd']),\n",
      " 'Center': set(['Civic Center']),\n",
      " 'Dr': set(['Entrada Dr', 'S Bundy Dr']),\n",
      " 'Ln': set(['Walnut Ln']),\n",
      " 'Pico': set(['West Pico']),\n",
      " 'Sepulveda': set(['1200 South Sepulveda']),\n",
      " 'St.': set(['Main St.']),\n",
      " 'access': set(['Santa Monica Pier access']),\n",
      " 'avenue': set(['veteran avenue']),\n",
      " 'broadway': set(['broadway'])}\n",
      "\n",
      "broadway => Broadway\n",
      "Civic Center => Civic Center Drive\n",
      "1200 South Sepulveda => 1200 South Sepulveda Boulevard\n",
      "veteran avenue => Veteran Avenue\n",
      "Main St. => Main Street\n",
      "Ocean Bd. => Ocean Boulevard\n",
      "Santa Monica Pier access => Santa Monica Pier Access\n",
      "Walnut Ln => Walnut Lane\n",
      "West Pico => West Pico Boulevard\n",
      "Santa Monica Bvd => Santa Monica Boulevard\n",
      "Pico Blvd => Pico Boulevard\n",
      "Wishire Blvd => Wishire Boulevard\n",
      "W Washington Blvd => West Washington Boulevard\n",
      "Wilshire Blvd => Wilshire Boulevard\n",
      "W Pico Blvd => West Pico Boulevard\n",
      "Santa Monica Blvd => Santa Monica Boulevard\n",
      "Ohio Ave => Ohio Avenue\n",
      "Montana Ave => Montana Avenue\n",
      "S Bundy Dr => South Bundy Drive\n",
      "Entrada Dr => Entrada Drive\n"
     ]
    }
   ],
   "source": [
    "st_types = audit(OSM_FILE)\n",
    "pprint.pprint(dict(st_types))\n",
    "print \"\"\n",
    "\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping_street, mapping_abbrev)\n",
    "        print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing problems and Preparing Data for Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so you will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "We've already provided the code needed to load the data, perform iterative parsing and write the\n",
    "output to csv files. Your task is to complete the shape_element function that will transform each\n",
    "element into the correct format. To make this process easier we've already defined a schema (see\n",
    "the schema.py file in the last code tab) for the .csv files and the eventual tables. Using the \n",
    "cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The final return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "The final return value for a \"way\" element should look something like:\n",
    "\n",
    "{'way': {'id': 209809850,\n",
    "         'user': 'chicago-buildings',\n",
    "         'uid': 674454,\n",
    "         'version': '1',\n",
    "         'timestamp': '2013-03-13T15:58:04Z',\n",
    "         'changeset': 15353317},\n",
    " 'way_nodes': [{'id': 209809850, 'node_id': 2199822281, 'position': 0},\n",
    "               {'id': 209809850, 'node_id': 2199822390, 'position': 1},\n",
    "               {'id': 209809850, 'node_id': 2199822392, 'position': 2},\n",
    "               {'id': 209809850, 'node_id': 2199822369, 'position': 3},\n",
    "               {'id': 209809850, 'node_id': 2199822370, 'position': 4},\n",
    "               {'id': 209809850, 'node_id': 2199822284, 'position': 5},\n",
    "               {'id': 209809850, 'node_id': 2199822281, 'position': 6}],\n",
    " 'way_tags': [{'id': 209809850,\n",
    "               'key': 'housenumber',\n",
    "               'type': 'addr',\n",
    "               'value': '1412'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street',\n",
    "               'type': 'addr',\n",
    "               'value': 'West Lexington St.'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:name',\n",
    "               'type': 'addr',\n",
    "               'value': 'Lexington'},\n",
    "              {'id': '209809850',\n",
    "               'key': 'street:prefix',\n",
    "               'type': 'addr',\n",
    "               'value': 'West'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:type',\n",
    "               'type': 'addr',\n",
    "               'value': 'Street'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building',\n",
    "               'type': 'regular',\n",
    "               'value': 'yes'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'levels',\n",
    "               'type': 'building',\n",
    "               'value': '1'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building_id',\n",
    "               'type': 'chicago',\n",
    "               'value': '366409'}]}\n",
    "\"\"\"\n",
    "\n",
    "OSM_PATH = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/santaMonica.osm\"\n",
    "\n",
    "NODES_PATH = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/nodes.csv\"\n",
    "NODE_TAGS_PATH = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/nodes_tags.csv\"\n",
    "WAYS_PATH = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/ways.csv\"\n",
    "WAY_NODES_PATH = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "# finding housenumber in addreses\n",
    "housenumber_in_street_re = re.compile(r'^(\\d)+(\\s)+')\n",
    "# finding ste or suite phrase in street address\n",
    "ste_re = re.compile(r'ste', re.IGNORECASE)\n",
    "suite_re = re.compile(r'suite', re.IGNORECASE)\n",
    "# an expression for catching state in postcode; like: CA 90405;\n",
    "state_in_postcode_re = re.compile(r'^([A-Z]){2}\\s{1}', re.IGNORECASE)\n",
    "\n",
    "# loading schema from schemaa file; The schemaa file is placed in the same directory that this notebook is placed;\n",
    "SCHEMA = schemaa.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def add_new_line(key, value, tags, element, types = 'addr'):\n",
    "    temp= {}\n",
    "    temp['id'] = element.attrib['id']\n",
    "    temp['key'] = key\n",
    "    temp['type'] = types\n",
    "    # using Uppercase for first letter of all words\n",
    "    temp['value'] = string.capwords(value)\n",
    "    tags.append(temp)\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handling secondary tags the same way for both node and way elements\n",
    "    \n",
    "    for el_tag in element.iter('tag'):\n",
    "        key_tag = el_tag.attrib['k']\n",
    "        \n",
    "        # for cleaning data from problematic characters\n",
    "        if problem_chars.search(key_tag):\n",
    "            continue\n",
    "        \n",
    "        # Fixing Street names\n",
    "        if is_street_name(el_tag):\n",
    "            \n",
    "            # updating problematic words with defined maps\n",
    "            el_tag.attrib['v'] = update_name(el_tag.attrib['v'], mapping_street, mapping_abbrev)\n",
    "            \n",
    "            # adding a new line for housenumber, if it's mentioned in street address\n",
    "            if housenumber_in_street_re.search(el_tag.attrib['v']):\n",
    "                value_list_1 = el_tag.attrib['v'].split(' ')\n",
    "                # keeping housenumber for creating a new line and deleting it from street address\n",
    "                housenumber = value_list_1.pop(0)\n",
    "                el_tag.attrib['v'] = ' '.join(value_list_1)\n",
    "                add_new_line('housenumber', housenumber, tags, element)\n",
    "            \n",
    "            # adding a new line for suite, if it's mentioned in street address\n",
    "            if (ste_re.search(el_tag.attrib['v'])) or (suite_re.search(el_tag.attrib['v'])):\n",
    "                value_list_2 = el_tag.attrib['v'].split(' ')\n",
    "                # keeping suite number for creating  a new line\n",
    "                suite_number = value_list_2.pop(-1)\n",
    "                # removing the suite word from the address\n",
    "                value_list_2.remove(value_list_2[-2])\n",
    "                el_tag.attrib['v'] = ' '.join(value_list_2)\n",
    "                # adding suite line\n",
    "                add_new_line('Suite', suite_number, tags, element)\n",
    "                \n",
    "        # Fixing House Numbers problems\n",
    "        if is_housenumber(el_tag):\n",
    "            # fixing the only housenumber that contains the whole address\n",
    "            if el_tag.attrib['v'].startswith('1850 Sawtelle Boulevard'):\n",
    "                value_list_3 = el_tag.attrib['v'].split(', ')\n",
    "                el_tag.attrib['v'] = value_list_3[0].split(' ')[0]\n",
    "                # adding suite line\n",
    "                add_new_line('Suite', value_list_3[1].split(' ')[-1], tags, element)\n",
    "                \n",
    "                # adding city line\n",
    "                add_new_line('city', value_list_3[2], tags, element)\n",
    "                \n",
    "                # adding state line\n",
    "                add_new_line('state', value_list_3[3].split(' ')[0], tags, element)\n",
    "                \n",
    "            \n",
    "            # adding a new line for suite, if it's mentioned in housenumber\n",
    "            elif (ste_re.search(el_tag.attrib['v'])) or (suite_re.search(el_tag.attrib['v'])):\n",
    "                value_list_2 = el_tag.attrib['v'].split(' ')\n",
    "                # keeping suite number for creating  a new line\n",
    "                suite_number = value_list_2[-1]\n",
    "                el_tag.attrib['v'] = value_list_2[0]\n",
    "                # adding new line for suite\n",
    "                add_new_line('Suite', suite_number, tags, element)\n",
    "                \n",
    "        # Fixing State\n",
    "        if is_state(el_tag):\n",
    "            el_tag.attrib['v'] = update_state(el_tag.attrib['v'], state_list)\n",
    "        \n",
    "        # Fixing Post Codes\n",
    "        if is_postcode(el_tag):\n",
    "            # finding problematic postcodes that contain state as part of postcode\n",
    "            if state_in_postcode_re.search(el_tag.attrib['v']):\n",
    "                value_list_4 = el_tag.attrib['v'].split(' ')\n",
    "                el_tag.attrib['v'] = value_list_4[1]\n",
    "                # adding new line for state\n",
    "                add_new_line('state', value_list_4[0], tags, element)\n",
    "                \n",
    "        # Fixing Phone numbers and format them \n",
    "        if is_phone(el_tag):\n",
    "            if el_tag.attrib['v'] in notValidPhones:\n",
    "                el_tag.attrib['v'] = el_tag.attrib['v'].replace('0', '+', 1)\n",
    "            # Uniform all phones the same like this: +13102606308\n",
    "            if el_tag.attrib['v'].startswith(\"+\"):\n",
    "                el_tag.attrib['v'] = el_tag.attrib['v'][1:]\n",
    "            z = phonenumbers.parse(el_tag.attrib['v'], \"US\") \n",
    "            el_tag.attrib['v'] = phonenumbers.format_number(z, phonenumbers.PhoneNumberFormat.E164)\n",
    "                \n",
    "        # changing key = 'url' to key = 'website'; because there are many \\\n",
    "        # website addresses and only 4 urls as keys;\n",
    "        if key_tag == 'url':\n",
    "            key_tag = 'website'\n",
    "        \n",
    "        # fixing problematic websites\n",
    "        if key_tag == 'source':\n",
    "            if el_tag.attrib['v'] in problemWebsite:\n",
    "                el_tag.attrib['v'] = el_tag.attrib['v'].split(',')[0]\n",
    "                \n",
    "        temp = {}\n",
    "        temp['id'] = element.attrib['id']\n",
    "        temp['key'] = key_tag\n",
    "        temp['type'] = default_tag_type           \n",
    "        temp['value'] = el_tag.attrib['v']\n",
    "        \n",
    "        if LOWER_COLON.search(key_tag):\n",
    "            key_tag_list = key_tag.split(\":\")\n",
    "            temp['type'] =  key_tag_list.pop(0)\n",
    "            temp['key'] = \":\".join(key_tag_list)\n",
    "            \n",
    "        if temp['type'] == 'addr':\n",
    "            # using Uppercase for first letter of all words in address related values; like street , city , ...\n",
    "            temp['value'] = string.capwords(temp['value'])\n",
    "        \n",
    "        tags.append(temp)\n",
    "        \n",
    "    if element.tag == 'node':\n",
    "        for field in node_attr_fields:\n",
    "            node_attribs[field] = element.attrib[field]\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "        for field in way_attr_fields:\n",
    "            way_attribs[field] = element.attrib[field]\n",
    "        counter = -1\n",
    "        for nd_tag in element.iter('nd'):\n",
    "            counter += 1\n",
    "            temp = {}\n",
    "            temp['id'] = element.attrib['id']\n",
    "            temp['node_id'] = nd_tag.attrib['ref']\n",
    "            temp['position'] = counter\n",
    "            way_nodes.append(temp)\n",
    "            \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "        codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "        codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "        codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "        codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_map(OSM_FILE, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Creating Database and it's Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a database with this name: openStreet.db\n",
    "db = sqlite3.connect('C:/Users/Shahrooz/Desktop/OpenStreetMap/DataSource_openStreet/openStreet.db')\n",
    "c = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating nodes Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x39a2c1e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_node_table = \"\"\"CREATE TABLE IF NOT EXISTS nodes(\n",
    "                                id INTEGER primary key,\n",
    "                                lat REAL,\n",
    "                                lon REAL,\n",
    "                                user INTEGER,\n",
    "                                uid TEXT,\n",
    "                                version TEXT,\n",
    "                                changeset INTEGER,\n",
    "                                timestamp TEXT\n",
    "                                );\"\"\"\n",
    "c.execute(create_node_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting Data from CSV file into nodes Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(NODES_PATH,'rb') as source:\n",
    "    dic = csv.DictReader(source) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['lat'],i['lon'], i['user'].decode(\"utf-8\"), i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dic]\n",
    "# insert the formatted data\n",
    "c.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14752647, 34.0278652, -118.4512786, u'techlady', u'104962', u'21', 11239147, u'2012-04-09T15:23:25Z'), (14752649, 34.0230505, -118.4746697, u'techlady', u'104962', u'20', 11245718, u'2012-04-09T22:09:46Z')]\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT id FROM nodes LIMIT 2;\"\n",
    "c.execute(query)\n",
    "rows = c.fetchall()\n",
    "print rows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Creating nodes_tags Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x39a2c1e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_node_tags_table = \"\"\"CREATE TABLE IF NOT EXISTS nodes_tags(\n",
    "                                id INTEGER references nodes(id),\n",
    "                                key TEXT,\n",
    "                                value TEXT,\n",
    "                                type TEXT\n",
    "                                );\"\"\"\n",
    "c.execute(create_node_tags_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting Data from CSV file into nodes_tags Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(NODE_TAGS_PATH,'rb') as source:\n",
    "    dic = csv.DictReader(source) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'],i['value'].decode(\"utf-8\"), i['type']) for i in dic]\n",
    "# insert the data to the table\n",
    "c.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating ways Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x39a2c1e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_way_table = \"\"\"CREATE TABLE IF NOT EXISTS ways(\n",
    "                                id INTEGER primary key,\n",
    "                                user INTEGER,\n",
    "                                uid TEXT,\n",
    "                                version TEXT,\n",
    "                                changeset INTEGER,\n",
    "                                timestamp TEXT\n",
    "                                );\"\"\"\n",
    "c.execute(create_way_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting Data from CSV file into ways Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(WAYS_PATH,'rb') as source:\n",
    "    dic = csv.DictReader(source) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['user'].decode(\"utf-8\"),i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dic]\n",
    "# insert the data to the table\n",
    "c.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating ways_nodes Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x39a2c1e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_way_nodes_table = \"\"\"CREATE TABLE IF NOT EXISTS ways_nodes(\n",
    "                                id INTEGER references ways(id),\n",
    "                                node_id INTEGER references nodes(id),\n",
    "                                position INTEGER\n",
    "                                );\"\"\"\n",
    "c.execute(create_way_nodes_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting Data from CSV file into ways_nodes Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(WAY_NODES_PATH,'rb') as source:\n",
    "    dic = csv.DictReader(source) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dic]\n",
    "# insert the data to the table\n",
    "c.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating ways_tags Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x39a2c1e0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_way_tags_table = \"\"\"CREATE TABLE IF NOT EXISTS ways_tags(\n",
    "                                id INTEGER references ways(id),\n",
    "                                key TEXT,\n",
    "                                value TEXT,\n",
    "                                type TEXT\n",
    "                                );\"\"\"\n",
    "c.execute(create_way_tags_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting Data from CSV file into ways_tags Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(WAY_TAGS_PATH,'rb') as source:\n",
    "    dic = csv.DictReader(source) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'].decode(\"utf-8\"), i['type']) for i in dic]\n",
    "# insert the data to the table\n",
    "c.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Data in Data Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looking at 'postcode' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'90025-9998',),\n",
      " (u'90025',),\n",
      " (u'90025',),\n",
      " (u'90401',),\n",
      " (u'90025',),\n",
      " (u'90403',),\n",
      " (u'90403',),\n",
      " (u'90402',),\n",
      " (u'90403',),\n",
      " (u'90403',),\n",
      " (u'90405',),\n",
      " (u'90404',),\n",
      " (u'90403',),\n",
      " (u'90402',),\n",
      " (u'90401',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90025',),\n",
      " (u'90025',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90025',),\n",
      " (u'90403',),\n",
      " (u'90403',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90025',),\n",
      " (u'90405',),\n",
      " (u'90401',),\n",
      " (u'90402',),\n",
      " (u'90405',),\n",
      " (u'90024',),\n",
      " (u'90292',),\n",
      " (u'90404',),\n",
      " (u'90401',),\n",
      " (u'90404',),\n",
      " (u'90405',),\n",
      " (u'90401',),\n",
      " (u'90291',),\n",
      " (u'90401',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90025',),\n",
      " (u'90025',),\n",
      " (u'90405',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90405',),\n",
      " (u'90401',),\n",
      " (u'90402',),\n",
      " (u'90404',),\n",
      " (u'90404',),\n",
      " (u'90401',),\n",
      " (u'90064',),\n",
      " (u'90404',),\n",
      " (u'90025',),\n",
      " (u'90025',),\n",
      " (u'90025',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90025',),\n",
      " (u'90404',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90403',),\n",
      " (u'90064',),\n",
      " (u'90403',),\n",
      " (u'90291',),\n",
      " (u'90064',),\n",
      " (u'90405',),\n",
      " (u'90501',),\n",
      " (u'90404',),\n",
      " (u'90291',),\n",
      " (u'90401',),\n",
      " (u'90291',),\n",
      " (u'90025',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90025',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90404',),\n",
      " (u'90404',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90291',),\n",
      " (u'90024',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90404',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90401',),\n",
      " (u'90403',),\n",
      " (u'90403',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90405',),\n",
      " (u'90404',),\n",
      " (u'90405',),\n",
      " (u'90064',),\n",
      " (u'90064',),\n",
      " (u'90405',),\n",
      " (u'90405',)]\n"
     ]
    }
   ],
   "source": [
    "query_post_code = \"SELECT value FROM nodes_tags WHERE key = 'postcode';\"\n",
    "c.execute(query_post_code)\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking 'postcode' for problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT * FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) as tags \\\n",
    "            WHERE (tags.value LIKE 'CA %' AND tags.key = 'postcode');\")\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for existance of 'url' as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT * FROM nodes_tags WHERE key = 'url';\")\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for fixed street name that contains housenumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT * FROM nodes_tags WHERE (key = 'street' AND value = '1200 South Sepulveda');\")\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Street Names Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Santa Monica Boulevard',),\n",
      " (u'Butler Avenue',),\n",
      " (u'Lincoln Boulevard',),\n",
      " (u'Wilshire Boulevard',),\n",
      " (u'Montana Avenue',),\n",
      " (u'Pico Boulevard',),\n",
      " (u'11th Street',),\n",
      " (u'Pacific Coast Highway',),\n",
      " (u'Ocean Front Walk',),\n",
      " (u'Main Street',),\n",
      " (u'South Sepulveda Boulevard',),\n",
      " (u'Santa Monica Place',),\n",
      " (u'Ocean Park Boulevard',),\n",
      " (u'Ohio Avenue',),\n",
      " (u'Third Street Promenade',),\n",
      " (u'Sawtelle Boulevard',),\n",
      " (u'West Pico Boulevard',),\n",
      " (u'Broadway',),\n",
      " (u'Entrada Drive',),\n",
      " (u'Westwood Boulevard',),\n",
      " (u'Glencoe Avenue',),\n",
      " (u'7th Street',),\n",
      " (u'Rose Avenue',),\n",
      " (u'Donald Douglas Loop South',),\n",
      " (u'Donald Douglas Loop North',),\n",
      " (u'Broadway Street',),\n",
      " (u'Ocean Boulevard',),\n",
      " (u'Stoner Avenue',),\n",
      " (u'Colorado Avenue',),\n",
      " (u'2nd Street',),\n",
      " (u'4th Street',),\n",
      " (u'South Barrington Avenue',),\n",
      " (u'15th Ste.',),\n",
      " (u'South Bundy Drive',),\n",
      " (u'Santa Monica Pier Access',),\n",
      " (u'Palisades Beach Road',),\n",
      " (u'South Carmelina Avenue',),\n",
      " (u'Abbot Kinney Boulevard',),\n",
      " (u'Breeze Avenue',),\n",
      " (u'Wishire Boulevard',),\n",
      " (u'West Olympic Boulevard',),\n",
      " (u'Sepulveda Boulevard',),\n",
      " (u'Colby Avenue',),\n",
      " (u'La Grange Avenue',),\n",
      " (u'Ocean Avenue',),\n",
      " (u'Southsepulveda Boulevard',),\n",
      " (u'Venice Boulevard',),\n",
      " (u'Berkeley Street',),\n",
      " (u'Maint Street',),\n",
      " (u'Franklin Street',),\n",
      " (u'20th Street',),\n",
      " (u'Santa Monica Suite',),\n",
      " (u'Walnut Lane',),\n",
      " (u'26th Street',),\n",
      " (u'Olympic Boulevard',),\n",
      " (u'Cloverfield Boulevard',),\n",
      " (u'Civic Center Drive',),\n",
      " (u'Capri Drive',),\n",
      " (u'West Washington Boulevard',),\n",
      " (u'16th Street',),\n",
      " (u'West Sunset Boulevard',),\n",
      " (u'Pearl Street',),\n",
      " (u'Barry Avenue',),\n",
      " (u'3rd Street',),\n",
      " (u'Ocean Way',),\n",
      " (u'15th Street',),\n",
      " (u'Michigan Avenue',),\n",
      " (u'Southbundy Drive',),\n",
      " (u'Airport Avenue',),\n",
      " (u'14th Street',),\n",
      " (u'California Avenue',),\n",
      " (u'18th Street',),\n",
      " (u'South Gretna Green Way',),\n",
      " (u'Brooks Avenue',),\n",
      " (u'Barnard Way',),\n",
      " (u'Pier Avenue',),\n",
      " (u'7th Avenue',),\n",
      " (u'Shell Avenue',),\n",
      " (u'San Vicente Boulevard',),\n",
      " (u'Moreno Avenue',),\n",
      " (u'Veteran Avenue',),\n",
      " (u'Beverly Avenue',),\n",
      " (u'Euclid Street',),\n",
      " (u'Ashland Avenue',)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT DISTINCT tags.value\\\n",
    "           FROM (SELECT * FROM nodes_tags UNION ALL\\\n",
    "                SELECT * FROM ways_tags) tags\\\n",
    "           WHERE key = 'street';\")\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities exist in Santa Monica Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Santa Monica', 142),\n",
      " (u'Los Angeles', 42),\n",
      " (u'Venice', 34),\n",
      " (u'Marina Del Rey', 6),\n",
      " (u'Los Angeles-venice', 3),\n",
      " (u'West Los Angeles', 2),\n",
      " (u'Pacific Palisades', 1),\n",
      " (u'Venice Ca', 1)]\n"
     ]
    }
   ],
   "source": [
    "city_query = \"\"\"SELECT tags.value, COUNT(*) as count\\\n",
    "                FROM (SELECT * FROM nodes_tags UNION ALL\\\n",
    "                SELECT * FROM ways_tags) tags\\\n",
    "                WHERE tags.key = 'city'\\\n",
    "                GROUP BY tags.value\\\n",
    "                ORDER BY count DESC;\"\"\"\n",
    "c.execute(city_query)\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which kind of amenities are in Santa Monica Region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'parking', 178),\n",
      " (u'restaurant', 90),\n",
      " (u'bicycle_rental', 81),\n",
      " (u'school', 55),\n",
      " (u'place_of_worship', 52),\n",
      " (u'cafe', 45),\n",
      " (u'fast_food', 24),\n",
      " (u'drinking_water', 23),\n",
      " (u'fuel', 22),\n",
      " (u'hospital', 17)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT tags.value, count(*) as count\\\n",
    "           FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) as tags \\\n",
    "           WHERE tags.key = 'amenity' \\\n",
    "           GROUP BY value \\\n",
    "           ORDER BY count DESC \\\n",
    "           LIMIT 10;\")\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which restaurant Cuisines are popular in this region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'american', 8),\n",
      " (u'italian', 5),\n",
      " (u'mexican', 3),\n",
      " (u'regional', 3),\n",
      " (u'burger', 2),\n",
      " (u'indian', 2),\n",
      " (u'sandwich', 2),\n",
      " (u'argentinian', 1),\n",
      " (u'asian', 1),\n",
      " (u'chinese', 1)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT nodes_tags.value, COUNT(*) as count \\\n",
    "           FROM (SELECT tags.id FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) \\\n",
    "           as tags \\\n",
    "           WHERE tags.value = 'restaurant') as restaurant_tags, nodes_tags\\\n",
    "           WHERE restaurant_tags.id = nodes_tags.id AND nodes_tags.key = 'cuisine'\\\n",
    "           GROUP BY nodes_tags.value\\\n",
    "           ORDER BY count DESC \\\n",
    "           LIMIT 10;\")\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which religions are related to Worship Places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'christian', 45), (u'buddhist', 2)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT nodes_tags.value, COUNT(*) as count\\\n",
    "           FROM (SELECT tags.id FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) as tags\\\n",
    "           WHERE tags.value = 'place_of_worship') as worship_place, nodes_tags\\\n",
    "           WHERE worship_place.id = nodes_tags.id AND nodes_tags.key = 'religion'\\\n",
    "           GROUP BY nodes_tags.value\\\n",
    "           ORDER BY count DESC;\")\n",
    "            \n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denomination of Worship Places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'baptist', 11),\n",
      " (u'methodist', 4),\n",
      " (u'catholic', 3),\n",
      " (u'lutheran', 3),\n",
      " (u'mormon', 2),\n",
      " (u'nichiren', 1),\n",
      " (u'presbyterian', 1)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT nodes_tags.value, COUNT(*) as count\\\n",
    "           FROM (SELECT tags.id FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) as tags\\\n",
    "           WHERE tags.value = 'place_of_worship') as worship_place, nodes_tags\\\n",
    "           WHERE worship_place.id = nodes_tags.id AND nodes_tags.key = 'denomination'\\\n",
    "           GROUP BY nodes_tags.value\\\n",
    "           ORDER BY count DESC;\")\n",
    "            \n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffee Shop brands in Santa Monica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Starbucks', 5),\n",
      " (u'Coffee Bean & Tea Leaf', 2),\n",
      " (u'Starbucks Coffee', 2),\n",
      " (u'Blue Bottle Venice', 1),\n",
      " (u'Bondi Harvest', 1)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT nodes_tags.value, COUNT(*) as count \\\n",
    "           FROM (SELECT tags.id FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) as tags\\\n",
    "           WHERE tags.value = 'cafe') as cafe, nodes_tags\\\n",
    "           WHERE cafe.id = nodes_tags.id AND nodes_tags.key = 'name'\\\n",
    "           GROUP BY nodes_tags.value\\\n",
    "           ORDER BY count DESC\\\n",
    "           LIMIT 5;\")\n",
    "            \n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Contributer Users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'kingrollo', 90092),\n",
      " (u'ridixcr_import', 73949),\n",
      " (u'Luis36995_labuildings', 71663),\n",
      " (u'manings_labuildings', 58496),\n",
      " (u'calfarome_labuilding', 56562),\n",
      " (u'schleuss_imports', 47011),\n",
      " (u'dannykath_labuildings', 42840),\n",
      " (u'Jothirnadh_labuildings', 35713),\n",
      " (u'saikabhi_LA_imports', 31338),\n",
      " (u'kingrollo_imports', 22558)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT user, COUNT(*) as count\\\n",
    "           FROM nodes\\\n",
    "           GROUP BY user\\\n",
    "           ORDER BY count DESC\\\n",
    "           LIMIT 10;\")\n",
    "            \n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Number of Users Contribute Only Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of one time Contributers:  85\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT user, COUNT(user) as count\\\n",
    "           FROM nodes\\\n",
    "           GROUP BY user \\\n",
    "           HAVING count = 1;\")\n",
    "            \n",
    "rows = c.fetchall()\n",
    "print \"Number of one time Contributers: \", len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common PostCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'90025', 186),\n",
      " (u'90291', 45),\n",
      " (u'90405', 45),\n",
      " (u'90404', 34),\n",
      " (u'90401', 33)]\n"
     ]
    }
   ],
   "source": [
    "c.execute(\"SELECT tags.value, COUNT(*) as count \\\n",
    "           FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) as tags\\\n",
    "           WHERE tags.key = 'postcode'\\\n",
    "           GROUP BY tags.value\\\n",
    "           ORDER BY count DESC\\\n",
    "           LIMIT 5;\")\n",
    "\n",
    "rows = c.fetchall()\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
